# -*- coding: utf-8 -*-
"""cic_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cLocxOkb4bCDixQhLffZnKzcCm9vsibj

# Imports necessary
"""

import pickle
import joblib

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers.schedules import ExponentialDecay

from google.colab import drive
import zipfile
drive.mount('/content/gdrive')

"""# Unzip files from drive"""

def unzip_file(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

zip_path = "/content/gdrive/MyDrive/CIC.zip"
extract_to = "/content/CIC"
unzip_file(zip_path, extract_to)

"""# Load and combine data sets"""

df_paths = []
for dirname, _, filenames in os.walk('/content/CIC/CIC'):
    for filename in filenames:
        df_paths.append(os.path.join(dirname, filename))

dataframes = []
for path in df_paths[1:5]:
    df = pd.read_csv(path)
    dataframes.append(df)

cdf = pd.concat(dataframes, ignore_index=True)

"""# Clean column names"""

cdf.columns = cdf.columns.str.replace(r'\s+', ' ', regex=True).str.strip().str.lower()
cdf.columns = cdf.columns.str.replace('/s', '', regex=False)

"""# Clean data set"""

columns_to_keep=[
    'destination port','total fwd packets','total backward packets','total length of fwd packets','total length of bwd packets','syn flag count','ack flag count','fin flag count','fwd packet length',
    'fwd packet length max','bwd packet length max','flow duration','init_win_bytes_forward','flow packets','down/up ratio','label'
]

columns_to_drop = [col for col in cdf.columns if col not in columns_to_keep]
cdf.drop(columns=columns_to_drop, inplace=True)

cdf.columns

"""# EDA"""

print(cdf.info())

cdf.head(10)

print(cdf.isna().sum())

print(f"Shape of the dataset: {cdf.shape}")

cdf.describe()

unique_values = cdf['label'].unique()
print(unique_values)

value_counts = cdf['label'].value_counts()
print(value_counts)

plt.figure(figsize=(10, 6))
label_counts = cdf['label'].value_counts().index
sns.countplot(x='label', data=cdf, palette='viridis', order=label_counts)

plt.xticks(rotation=90)
plt.title("Label Distribution")
plt.show()

"""# Encoding the values"""

# Filter classes with sufficient samples
label_counts = cdf['label'].value_counts()
labels_to_keep = label_counts[label_counts > 10000].index
cdf = cdf[cdf['label'].isin(labels_to_keep)]

# Encode labels
encoder = LabelEncoder()
cdf['label'] = encoder.fit_transform(cdf['label'])
class_names = encoder.classes_

"""# Train-Test split"""

X = cdf.drop('label', axis=1)
y = cdf['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    stratify=y,
    random_state=42
)

# Check for infinities or NaNs in training data
print("Infinities in X_train:", np.isinf(X_train).any())
print("-"*20)
print("NaNs in X_train:", np.isnan(X_train).any())
print("-" * 20)
# Check test data too
print("Infinities in X_test:", np.isinf(X_test).any())
print("-"*20)
print("NaNs in X_test:", np.isnan(X_test).any())
print("-"*20)
print("Infinities in y_train:", np.isinf(y_train).any())
print("-"*20)
print("NaNs in y_train:", np.isnan(y_train).any())
print("-"*20)
# Check test data too
print("Infinities in y_test:", np.isinf(y_test).any())
print("-"*20)
print("NaNs in y_test:", np.isnan(y_test).any())

"""# Agument data by replacing inf with nan and them adding means"""

import numpy as np
from sklearn.impute import SimpleImputer

# Replace infinities with NaN
X_train['flow packets'] = X_train['flow packets'].replace([np.inf, -np.inf], np.nan)
X_test['flow packets'] = X_test['flow packets'].replace([np.inf, -np.inf], np.nan)

# Impute NaN values with the mean
imputer = SimpleImputer(strategy='mean')
X_train['flow packets'] = imputer.fit_transform(X_train[['flow packets']])
X_test['flow packets'] = imputer.transform(X_test[['flow packets']])

"""# Scale the data with z-norm"""

# Scale data properly
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Oversampling or UnderSampling for balancing out the data"""

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(sampling_strategy='auto', random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)

"""# Set up the Model"""

def create_model(input_shape,n_classes):
  model= Sequential(
      [
          BatchNormalization(input_shape=input_shape),
          Dense(256, activation='swish',kernel_regularizer=l2(1e-4)),
          Dropout(0.3),
          BatchNormalization(),
          Dense(128, activation='swish',kernel_regularizer=l2(1e-5)),
          Dropout(0.2),
          Dense(n_classes,activation='softmax')
      ]
  )
  return model

model = create_model(input_shape=X_resampled.shape[1:],n_classes=len(class_names))

"""# Compile the mdoel"""

initial_learning_rate= 0.001
lr_schedule = ExponentialDecay(
    initial_learning_rate,
    decay_steps=10000,
    decay_rate=0.9,
    staircase=True
)

early_stop = EarlyStopping(
    monitor='val_accuracy',
    patience=5,
    restore_best_weights=True,
    mode='max'
)

model.compile(
    optimizer=Adam(learning_rate=lr_schedule),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

"""# Train the model"""

history = model.fit(
    X_resampled, y_resampled,
    validation_data=(X_test_scaled, y_test),
    epochs=100,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

"""# Training analysis"""

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Progress')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Progress')
plt.legend()
plt.show()

"""# Predictions"""

# Generate predictions
y_pred_probs = model.predict(X_test_scaled)
y_pred = np.argmax(y_pred_probs, axis=1)

# Convert encoded labels back to original names
y_test_labels = encoder.inverse_transform(y_test)
y_pred_labels = encoder.inverse_transform(y_pred)

"""# Accuracy and recall pricission scores"""

print("\nClassification Report:")
print(classification_report(y_test_labels, y_pred_labels, target_names=class_names))

# Confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(confusion_matrix(y_test_labels, y_pred_labels),
            annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names,
            yticklabels=class_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# Save model weights and scalars"""

model.save('/content/gdrive/MyDrive/cic_trained_model.h5')

# Save class names
with open('/content/gdrive/MyDrive/cic_class_names.pkl', 'wb') as f:
    pickle.dump(class_names, f)

# Save the StandardScaler
with open('/content/gdrive/MyDrive/cic_scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# Save model architecture
with open('/content/gdrive/MyDrive/cic_model_architecture.json', 'w') as f:
    f.write(model.to_json())

# Save model weights
model.save_weights('/content/gdrive/MyDrive/cic_model.weights.h5')